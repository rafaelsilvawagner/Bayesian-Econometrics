########################################################################################################################################
############################################################LOAD PACKAGES###############################################################
########################################################################################################################################

require(xts)
require(data.table)
require(highfrequency)
library(caret)
require(forecast)
require(RWeka)
require(randomGLM)
require(Cubist)
require(colf)
require(xgboost)
require(MCS)
require(glmnet)
require(MASS)
require(Rsolnp)
require(foreach)
require(doParallel)
require(mvtnorm)
require(beepR)
require(glmc)
require(doParallel)



########################################################################################################################################
############################################################PREPARE THE DATA############################################################
########################################################################################################################################

#Asset tickers
assets<-c("AAPL","EBAY","GS","MSFT","PG","ABT","BAC","CVX","GE","CAT")

#Let's make the first asset RV
data <- read.table(sprintf("C://Users//Usuario//Desktop//Rafael - Dissertacao//%s.txt",assets[1]), header = TRUE, sep = ',', dec = '.')
date<- as.POSIXct(paste(data$Date, data$Time), format="%m/%d/%Y %H:%M")
xts <- xts(data[,-1], order.by=date)
realizedVar<-rCov(xts$Close,cor = FALSE, align.by = 'minutes', #YOU'LL ANALIXE 5 MIN REALIZED VARIANCE
                  align.period = 5,   makeReturns = TRUE)
index(realizedVar)<-as.Date(index(realizedVar))#like all xts objects make a index




#take a look in your frame
#View(realizedVar)

#Make the same procedure for all the assets defined in the assets frame
for(asset in assets[-1]){
  data <- read.table(sprintf("C://Users//Usuario//Desktop//Rafael - Dissertacao//%s.txt",asset), header = TRUE, sep = ',', dec = '.')
  date<- as.POSIXct(paste(data$Date, data$Time), format="%m/%d/%Y %H:%M")
  xts <- xts(data[,-1], order.by=date)
  realized<-rCov(xts$Close,cor = FALSE, align.by = 'minutes', 
                 align.period = 5,   makeReturns = TRUE)
  index(realized)<-as.Date(index(realized))
  
  realizedVar<-merge.xts(realizedVar,realized, join='inner')
}

colnames(realizedVar)<-assets

#Pay attention, we will analyze the log of realizedVar
realizedVar<-log((realizedVar))
forecasts_1step<-list()
forecasts_5step<-list()

#take a look in your frame
#View(realizedVar)


########################################################################################################################################
#############################################################BAYESIAN STUFF#############################################################
########################################################################################################################################

#Makes prior for 22 lags plus an intercept
B0<-rep(0,23)
#lets say that the HAR defs has expectation of be 0.3
B0[c(2,6,23)]<-0.3
#lets say that the HAR coefficient has expectation of be 0.01
B0[c(1)]<-0.01
V0<-c(0.2,0.2,(sin(seq(pi,2*pi,pi/4))[-c(1,5)]+1.1)*0.0001,0.2,
      (sin(seq(pi,2*pi,pi/17))[-c(1,18)]+1.1)*0.0001,0.2)

simu_prior<-matrix(nrow=10000,ncol=23)

for(i in 1:nrow(simu_prior)){
  simu_prior[i,]<-rmvnorm(1,B0,diag(V0))
}

boxplot(simu_prior)

V0<-diag(V0)
s0<-1/2
v0<-4

########################################################################################################################################
#############################################################FREQUENTIST STUFF##########################################################
########################################################################################################################################

#You'll do everything for all assets
#for(asset in 1:length(assets)){
for(asset in 9:10){
  
  #catch the realized Var for the asset
  Var<-realizedVar[,asset]
  reg<-cbind(Var,matrix(NA,nrow=nrow(Var),ncol=22))
  #create Mean Lags
  for(j in 2:23){
    for(m in j:nrow(reg)){
      reg[m,j]<-mean(Var[(m-1):(m-j+1)])
    }
  }
  
  
  reg<-reg[23:nrow(reg),]
  reg_1<-reg
  reg_5<-cbind(lag(reg[,1],-4),reg[1:(nrow(reg)-4),-1])
  
  #give lags names
  colnames(reg_1)<-c('Close',paste("Close.",1:22,sep=""))
  colnames(reg_5)<-c('Close',paste("Close.",6:27,sep=""))
  reg_5<-reg_5[1:(nrow(reg_5)-4),]
  index(reg_5)<-index(reg_1)[5:nrow(reg_1)]
  
  #Create matrix to store the results
  models<-6
  comb<-6
  
  initial_sample_1<-round(0.8*nrow(reg_1))
  total_forecasts_1<-nrow(reg_1)-initial_sample_1
  
  predicted_1step<-matrix(NA,nrow=total_forecasts_1,ncol=(models+comb+1))
  predicted_1step<-xts(x=predicted_1step,order.by=index(reg_1)[(initial_sample_1+1):(nrow(reg_1))])
  
  initial_sample_5<-round(0.8*nrow(reg_5))
  total_forecasts_5<-nrow(reg_5)-initial_sample_5
  
  predicted_5step<-matrix(NA,nrow=total_forecasts_5,ncol=(models+comb+1))
  predicted_5step<-xts(x=predicted_5step,order.by=index(reg_5)[(initial_sample_5+1):(nrow(reg_5))])
  
  
  colnames(predicted_1step)<-c("Tree","Bagging","HAR","Cubist","Lasso","Bayes","Comb_MSE_Pos","Comb_QLIKE_Pos",
                               "Comb_MSE_Pos_1","Comb_QLIKE_Pos_1","DMA_MSE","DMA_QLIKE","Realized")
  
  colnames(predicted_5step)<-c("Tree","Bagging","HAR","Cubist","Lasso","Bayes","Comb_MSE_Pos","Comb_QLIKE_Pos",
                               "Comb_MSE_Pos_1","Comb_QLIKE_Pos_1","DMA_MSE","DMA_QLIKE","Realized")
  
  
  #Start the dinamic moving average weights
  dma_weigths_MSE<-rep(1/(models),models)
  dma_weigths_QLIKE<-rep(1/(models),models)
  forget_rate=0.95
  
  #Predict for all the days
  for(ri in initial_sample_1:(nrow(reg_1)-1)){
    
    ################################################################################################################################
    #################################################PREVISÕES 1 PASSO##############################################################
    ################################################################################################################################
    
    #In the first day store the realized
    if(ri==initial_sample_1){
      predicted_1step[,ncol(predicted_1step)]<-exp(reg_1[(initial_sample_1+1):(nrow(reg_1)),1])
    }
    
    #Select the data to train
    yx<-(reg_1[(ri+1-initial_sample_1):ri, ])
    #Separate the regressors
    x<-(reg_1[(ri+1-initial_sample_1):ri,-1])
    #Separate the response
    y<-(reg_1[(ri+1-initial_sample_1):ri, 1])
    
    #Fit M5 model with pruned trees and smoothe predictions
    M5_model = RWeka::M5P(Close ~ ., data = yx,control=Weka_control(N=F,U=F,R=F))
    
    
    #Make bagging fitting M5 models as base learners
    M5_bag = Bagging(Close ~ ., data = yx, 
                     control = Weka_control(P=100,I=200,W = list("weka.classifiers.trees.M5P", N=F,U=T,R=F,M=10)) )
    
    #cv_test<-matrix(nrow=500,ncol=500)
    #cv<-2/3
    #for(l in seq(10,100,10)){
    #  print(l)
    #  for(c in seq(50,500,50)){
    #    print(paste(l,c))
    #    test<-base::sample(1:nrow(y),1/3*nrow(y))
    #    train<-which(!(1:length(y)%in%test))
    #    y_train<-y[train,]
    #    x_train<-x[train,]
          
    #      y_test<-y[which(1:length(y)%in%test),]
    #      
    #      x_test<-x[which(1:length(y)%in%test),]
          
    #      M5_bag = Bagging(Close ~ ., data = yx, 
    #                       control = Weka_control(P=l,I=c,W = list("weka.classifiers.trees.M5P", N=F,U=T,R=F,M=10)) )
    #    cv_test[l,c]<-mean((predict(M5_bag,x_test)-y_test)^2)
    #    cv_test[l,c]<-sqrt(var((predict(M5_bag,x_test)-y_test)^2))
    #    }
     #}
    
    #n_min<-which.min(cv_test[,1])
    #cv_test[,1]<cv_test[200,1]+cv_test[n_min,2]
    #cv_test[seq(10,100,10),seq(50,500,50)]<0.4638382
    #cv_test[400,250]
    ##400 250 .4638382
    #require(rgl)
    #  open renderer
    #open3d()
    #  plot surface
    #rgl.surface(seq(10,200,10), seq(5,100,5),   cv_test[seq(5,100,5),seq(10,200,10)]*10000)
    #  Export to png
    #rgl.snapshot( "sample.png" , fmt="png", top=TRUE )
    #require(plot3D)
    
    #persp3D(z = cv_test[seq(50,500,50),seq(50,500,50)], theta = 120)
    #To see M5P options please look at
    #WOW(M5P)
    
    #Fits the HAR Model
    ls<-lm(Close~Close.1+Close.5+Close.22, 
           data = yx)
    
    #Fits Cubist Model
    cube<-cubist(x=as.matrix(x),y=as.matrix(y),committees=5, neighbors=5)
   
    #cv_test<-matrix(nrow=100,ncol=1)
    #cv<-2/3
    ##for(l in seq(5,100,5)){
    #  print(l)
    #  for(c in 5){
    #      test<-base::sample(1:nrow(y),1/3*nrow(y))
    #      train<-which(!(1:length(y)%in%test))
    #      y_train<-y[train,]
    #      x_train<-x[train,]
    #      
    #      y_test<-y[which(1:length(y)%in%test),]
    #      x_test<-x[which(1:length(y)%in%test),]
    #      
    ##      cube<-cubist(x=as.matrix(x_train),y=as.matrix(y_train),committees=l, neighbors=c)
    #      cv_test[l,1]<-mean((predict(cube,x_test)-y_test)^2)
    #      cv_test[l,1]<-sqrt(var((predict(cube,x_test)-y_test)^2))
    #    }
    #}
    
    #plot(cv_test[seq(5,100,5),1])
    #n_min<-which.min(cv_test[,1])
    #cv_test[,1]<cv_test[200,1]+cv_test[n_min,2]
    #cv_test[seq(5,100,5),seq(5,200,5)]
    
    #require(rgl)
    ##  open renderer
    #open3d()
    #  plot surface
    #rgl.surface(seq(10,200,10), seq(5,100,5),   cv_test[seq(5,100,5),seq(10,200,10)]*10000)
    #  Export to png
    #rgl.snapshot( "sample.png" , fmt="png", top=TRUE )
    #require(plot3D)
    
    #persp3D(z = cv_test[seq(5,100,5),seq(10,200,10)], theta = 120)
    
    ## Ridge Regression to create the Adaptive Weights Vector
    #cv.ridge <- cv.glmnet(as.matrix(x), as.matrix(y),parallel=TRUE, alpha=0, standardize=TRUE)
    #w3 <- 1/abs(matrix(coef(cv.ridge, s=cv.ridge$lambda.min)
    #                   [, 1][2:(ncol(x)+1)] ))^1 ## Using gamma = 1
    #w3[w3[,1] == Inf] <- 999999999 ## Replacing values estimated as Infinite for 999999999
    
    ## Adaptive Lasso
    #cv.out <- cv.glmnet(as.matrix(x), as.matrix(y), alpha=1, parallel=TRUE, standardize=TRUE, penalty.factor=w3)
    cv.out <- cv.glmnet(x=as.matrix(x), y=as.matrix(y), alpha=1)
    
    #Fits Lasso regression model
    lasso.mod <- glmnet(x=as.matrix(x), y=as.matrix(y), lambda=cv.out$lambda.1se,
                        alpha=1)
    
    #Make the bayesian estimation
    x_bayes<-cbind(1,x)
    bols<-solve(t(x_bayes)%*%x_bayes)%*%t(x_bayes)%*%y
    
    V1<-solve(solve(V0)+t(x_bayes)%*%x_bayes)
    B1<-V1%*%(solve(V0)%*%B0+t(x_bayes)%*%x_bayes%*%bols)
    
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1] = exp(predict(M5_model, (reg_1[(ri+1),])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),2] = exp(predict(M5_bag, (reg_1[(ri+1),])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),3] = exp(predict(ls, (reg_1[(ri+1),])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),4] = exp(predict(cube, as.matrix((reg_1[(ri+1),-1]))))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),5] = exp(predict(lasso.mod, as.matrix((reg_1[(ri+1),-1]))))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),6] = exp((c(1,matrix(reg_1[(ri+1),-1]))%*%B1))
    
    
    in_sample<-as.matrix((reg_1[(ri-round(0.8*nrow(reg_1))+1):ri,1]))*0
    in_sample<-cbind(in_sample,in_sample,in_sample,in_sample,in_sample,in_sample,in_sample)
    colnames(in_sample)<-c("M5P","Bagging","HAR","Cubist","Lasso","Bayesian","Realized")
    
    in_sample[,1]<-exp((predict(M5_model, (reg_1[(ri+1-initial_sample_1):ri,]))))
    in_sample[,2]<-exp((predict(M5_bag, (reg_1[(ri+1-initial_sample_1):ri,]))))
    in_sample[,3]<-exp((predict(ls, (reg_1[(ri+1-initial_sample_1):ri,]))))
    in_sample[,4]<-exp((predict(cube, (reg_1[(ri+1-initial_sample_1):ri,]))))
    in_sample[,5]<-exp((predict(lasso.mod, as.matrix((reg_1[(ri+1-initial_sample_1):ri,-1])))))
    in_sample[,6]<-exp(((as.matrix(cbind(1,reg_1[(ri+1-initial_sample_1):ri,-1])))%*%B1))
    in_sample[,7]<-exp(y) 
    
    
    
    #QLIKE
    QLIKE <- function(x) {
      reta<-x[1]+ in_sample[,1]*x[2] +in_sample[,2]*x[3] +in_sample[,3]*x[4] +
        in_sample[,4]*x[5] +in_sample[,5]*x[6]+in_sample[,6]*x[7]
      
      sum(in_sample[,7]/reta-log(in_sample[,7]/reta)-1)
    }
    
    
    
    MSE <- function(x) {
      reta<-x[1]+ in_sample[,1]*x[2] +in_sample[,2]*x[3] +in_sample[,3]*x[4] +
        in_sample[,4]*x[5] +in_sample[,5]*x[6]+in_sample[,6]*x[7]
      
      sum((reta-in_sample[,7])^2)
    }
    
    
    equal <- function(x) {
      x[2]+x[3]+x[4]+x[5]+x[6]+x[7]
    }
    
    par_MSE_POS<-solnp(pars=c(0,1/6,1/6,1/6,1/6,1/6,1/6), #starting values (random - obviously need to be positive and sum to 15)
                       fun=MSE,
                       LB=c(-10,0,0,0,0,0,0), 
                       control=list(tol=.Machine$double.eps)
    )$pars
    
    par_QLIKE_POS<-solnp(pars=c(1,1,1,1,1,1,1), #starting values (random - obviously need to be positive and sum to 15)
                         fun=QLIKE,
                         LB=c(-10,0,0,0,0,0,0), #lower bound for parameters i.e. greater than zero
                         control=list(tol=.Machine$double.eps)
    )$pars
    
    par_MSE_POS_1<-solnp(pars=c(0,0.2,0.2,0.2,0.2,0.1,0.1), #starting values (random - obviously need to be positive and sum to 15)
                         fun=MSE,
                         LB=c(-1,0,0,0,0,0,0), 
                         eqfun=equal, #equality function 
                         eqB=1,   #the equality constraint
                         control=list(tol=.Machine$double.eps)
    )$pars
    
    par_QLIKE_POS_1<-solnp(pars=c(0,0.2,0.2,0.2,0.2,0.1,0.1), #starting values (random - obviously need to be positive and sum to 15)
                           fun=QLIKE,
                           LB=c(-1,0,0,0,0,0,0),
                           eqfun=equal, #equality function 
                           eqB=1,   #the equality constraint
                           control=list(tol=.Machine$double.eps)
    )$pars
    
    if(ri==initial_sample_1){
      dma_weigths_MSE<-dma_weigths_MSE
      dma_weigths_QLIKE<-dma_weigths_QLIKE
    } else{
      real<-rep((predicted_1step[(ri-round(0.8*nrow(reg))),ncol(predicted_1step)]),models)
      fore<-(predicted_1step[(ri-round(0.8*nrow(reg))),1:6])
      
      erro_MSE<-(fore-real)^2
      #tinha um erro aqui
      sum1   = dma_weigths_MSE%*%(t(erro_MSE)^-1)
      pi_t_k = (as.numeric(dma_weigths_MSE)/c(erro_MSE))/as.numeric(sum1)
      sum2   = sum(pi_t_k^(forget_rate))
      dma_weigths_MSE = pi_t_k^(forget_rate)/as.numeric(sum2)
      
      erro_QLIKE<-(real)/(fore)-log((real)/(fore))-1
      sum1   = as.numeric(dma_weigths_QLIKE)%*%(t(erro_QLIKE)^-1)
      pi_t_k = as.numeric(dma_weigths_QLIKE)/erro_QLIKE/as.numeric(sum1)
      sum2   = sum(pi_t_k^(forget_rate))
      dma_weigths_QLIKE = pi_t_k^(forget_rate)/as.numeric(sum2)
    }
    
    
    
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),7]<-sum(c(par_MSE_POS)*(c(1,predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1:6])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),8]<-sum(c(par_QLIKE_POS)*(c(1,predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1:6])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),9]<-sum(c(par_MSE_POS_1)*(c(1,predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1:6])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),10]<-sum(c(par_QLIKE_POS_1)*(c(1,predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1:6])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),11]<-(dma_weigths_MSE%*%t((predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1:6])))
    predicted_1step[(ri-round(0.8*nrow(reg_1))+1),12]<-(dma_weigths_QLIKE%*%t((predicted_1step[(ri-round(0.8*nrow(reg_1))+1),1:6])))
  }
  
  
  dma_weigths_MSE<-rep(1/(models),models)
  dma_weigths_QLIKE<-rep(1/(models),models)
  forget_rate=0.95
  
  
  for(ri in initial_sample_5:(nrow(reg_5)-1)){
    
    ################################################################################################################################
    #################################################PREVISÕES 5 PASSO##############################################################
    ################################################################################################################################
    
    #In the first day store the realized
    if(ri==initial_sample_5){
      predicted_5step[,ncol(predicted_5step)]<-exp(reg_5[(initial_sample_5+1):(nrow(reg_5)),1])
    }
    
    #Select the data to train
    yx<-(reg_5[(ri+1-initial_sample_5):ri, ])
    #Separate the regressors
    x<-(reg_5[(ri+1-initial_sample_5):ri,-1])
    #Separate the response
    y<-(reg_5[(ri+1-initial_sample_5):ri, 1])
    
    
    #Fit M5 model with pruned trees and smoothe predictions
    M5_model = RWeka::M5P(Close ~ ., data = yx,control=Weka_control(N=F,U=F,R=F))
    
    #Make bagging fitting M5 models as base learners
    M5_bag = Bagging(Close ~ ., data = yx, 
                     control = Weka_control(P=100,I=200,W = list("weka.classifiers.trees.M5P", N=F,U=T,R=F,M=10)) )
    #To see M5P options please look at
    #WOW(M5P)
    
    #Fits the HAR Model
    ls<-lm(Close~Close.6+Close.10+Close.27, 
           data = yx)
    
    #Fits Cubist Model
    cube<-cubist(x=as.matrix(x),y=as.matrix(y),committees=5, neighbors=5)
   #cubistTuned <- caret::train(x=data.frame(x),y=data.frame(as.numeric(y)), method = "cubist")
    
    #LASSO
    cv.out <- cv.glmnet(x=as.matrix(x), y=as.matrix(y), alpha=1)
    
    #Fits Lasso regression model
    lasso.mod <- glmnet(x=as.matrix(x), y=as.matrix(y), lambda=cv.out$lambda.1se,
                        alpha=1)
    
    
    #Make the bayesian estimation
    x_bayes<-cbind(1,x)
    bols<-solve(t(x_bayes)%*%x_bayes)%*%t(x_bayes)%*%y
    
    V1<-solve(solve(V0)+t(x_bayes)%*%x_bayes)
    B1<-V1%*%(solve(V0)%*%B0+t(x_bayes)%*%x_bayes%*%bols)
    
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1] = exp(predict(M5_model, (reg_5[(ri+1),])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),2] = exp(predict(M5_bag, (reg_5[(ri+1),])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),3] = exp(predict(ls, (reg_5[(ri+1),])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),4] = exp(predict(cube, as.matrix((reg_5[(ri+1),-1]))))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),5] = exp(predict(lasso.mod, as.matrix((reg_5[(ri+1),-1]))))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),6] = exp((c(1,matrix(reg_5[(ri+1),-1]))%*%B1))
    
    
    in_sample<-as.matrix((reg_5[(ri-initial_sample_5+1):ri,1]))*0
    in_sample<-cbind(in_sample,in_sample,in_sample,in_sample,in_sample,in_sample,in_sample)
    colnames(in_sample)<-c("M5P","Bagging","HAR","Cubist","Lasso","Bayesian","Realized")
    
    
    in_sample[,1]<-exp((predict(M5_model, (reg_5[(ri+1-initial_sample_5):ri,]))))
    in_sample[,2]<-exp((predict(M5_bag, (reg_5[(ri+1-initial_sample_5):ri,]))))
    in_sample[,3]<-exp((predict(ls, (reg_5[(ri+1-initial_sample_5):ri,]))))
    in_sample[,4]<-exp((predict(cube, (reg_5[(ri+1-initial_sample_5):ri,]))))
    in_sample[,5]<-exp((predict(lasso.mod, as.matrix((reg_5[(ri+1-initial_sample_5):ri,-1])))))
    in_sample[,6]<-exp(((as.matrix(cbind(1,reg_5[(ri+1-initial_sample_5):ri,-1])))%*%B1))
    in_sample[,7]<-exp((reg_5[(ri+1-initial_sample_5):ri,1])) 
    
    
    
    #QLIKE
    QLIKE <- function(x) {
      reta<-x[1]+ in_sample[,1]*x[2] +in_sample[,2]*x[3] +in_sample[,3]*x[4] +
        in_sample[,4]*x[5] +in_sample[,5]*x[6]++in_sample[,6]*x[7]
      
      sum(in_sample[,7]/reta-log(in_sample[,7]/reta)-1)
    }
    
    
    
    MSE <- function(x) {
      reta<-x[1]+ in_sample[,1]*x[2] +in_sample[,2]*x[3] +in_sample[,3]*x[4] +
        in_sample[,4]*x[5] +in_sample[,5]*x[6]+in_sample[,6]*x[7]
      
      sum((reta-in_sample[,7])^2)
    }
    
    
    equal <- function(x) {
      x[2]+x[3]+x[4]+x[5]+x[6]+x[7]
    }
    
    par_MSE_POS<-solnp(pars=c(0,1/6,1/6,1/6,1/6,1/6,1/6), #starting values (random - obviously need to be positive and sum to 15)
                       fun=MSE,
                       LB=c(-10,0,0,0,0,0,0), #lower bound for parameters i.e. greater than zero
                       control=list(tol=.Machine$double.eps)
    )$pars
    
    par_QLIKE_POS<-solnp(pars=c(1,1,1,1,1,1,1), #starting values (random - obviously need to be positive and sum to 15)
                         fun=QLIKE,
                         LB=c(-10,0,0,0,0,0,0), #lower bound for parameters i.e. greater than zero
                         control=list(tol=.Machine$double.eps)
    )$pars
    
    par_MSE_POS_1<-solnp(pars=c(0,0.2,0.2,0.2,0.2,0.1,0.1), #starting values (random - obviously need to be positive and sum to 15)
                         fun=MSE,
                         LB=c(-1,0,0,0,0,0,0), 
                         eqfun=equal, #equality function 
                         eqB=1,   #the equality constraint
                         control=list(tol=.Machine$double.eps)
    )$pars
    
    par_QLIKE_POS_1<-solnp(pars=c(0,0.2,0.2,0.2,0.2,0.1,0.1), #starting values (random - obviously need to be positive and sum to 15)
                           fun=QLIKE,
                           LB=c(-1,0,0,0,0,0,0), 
                           eqfun=equal, #equality function 
                           eqB=1,   #the equality constraint
                           control=list(tol=.Machine$double.eps)
    )$pars
    
    
    if(ri==initial_sample_5){
      dma_weigths_MSE<-dma_weigths_MSE
      dma_weigths_QLIKE<-dma_weigths_QLIKE
    } else{
      real<-rep((predicted_5step[(ri-initial_sample_5),ncol(predicted_5step)]),models)
      fore<-(predicted_5step[(ri-initial_sample_5),1:6])
      
      erro_MSE<-(fore-real)^2
      sum1   = dma_weigths_MSE%*%(t(erro_MSE)^-1)
      pi_t_k = ((as.numeric(dma_weigths_MSE)/c(erro_MSE)))/as.numeric(sum1)
      sum2   = sum(pi_t_k^(forget_rate))
      dma_weigths_MSE = pi_t_k^(forget_rate)/as.numeric(sum2)
      
      erro_QLIKE<-(real)/(fore)-log((real)/(fore))-1
      sum1   = as.numeric(dma_weigths_QLIKE)%*%(t(erro_QLIKE)^-1)
      pi_t_k = (as.numeric(dma_weigths_QLIKE)/erro_QLIKE)/as.numeric(sum1)
      sum2   = sum(pi_t_k^(forget_rate))
      dma_weigths_QLIKE = pi_t_k^(forget_rate)/as.numeric(sum2)
    }
    
    
    
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),7]<-sum(c(par_MSE_POS)*(c(1,predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1:6])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),8]<-sum(c(par_QLIKE_POS)*(c(1,predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1:6])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),9]<-sum(c(par_MSE_POS_1)*(c(1,predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1:6])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),10]<-sum(c(par_QLIKE_POS_1)*(c(1,predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1:6])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),11]<-(dma_weigths_MSE%*%t((predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1:6])))
    predicted_5step[(ri-round(0.8*nrow(reg_5))+1),12]<-(dma_weigths_QLIKE%*%t((predicted_5step[(ri-round(0.8*nrow(reg_5))+1),1:6])))
  }
  
  
  write.csv(index(predicted_1step),"C://Users//Usuario//Desktop//Rafael - Dissertacao//Realized Variance//Combinations//Dates_1.csv")
  write.csv(index(predicted_5step),"C://Users//Usuario//Desktop//Rafael - Dissertacao//Realized Variance//Combinations//Dates_5.csv")
  
  write.csv(predicted_1step,sprintf("C://Users//Usuario//Desktop//Rafael - Dissertacao//Realized Variance//Combinations//predicted_1step_%s.csv",asset))
  write.csv(predicted_5step,sprintf("C://Users//Usuario//Desktop//Rafael - Dissertacao//Realized Variance//Combinations//predicted_5step_%s.csv",asset))
}



